{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd37e5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbbd033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "\n",
    "from src.algorithms.parallel_jax_pso import parallel_jax_pso\n",
    "from src.config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488a31ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aabb5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_algorithm(\n",
    "    algorithm_name: str,\n",
    "    algorithm_fn: callable,\n",
    "    objective_fn: callable,\n",
    "    bounds: tuple,\n",
    "    params: dict,\n",
    "    num_runs: int,\n",
    "    num_subswarms: int,\n",
    ") -> float:\n",
    "    execution_times = []\n",
    "\n",
    "    for i in range(num_runs):\n",
    "        logger.info(\n",
    "            '%s | Run %d/%d | [RUNNING]',\n",
    "            algorithm_name,\n",
    "            i + 1,\n",
    "            num_runs,\n",
    "        )\n",
    "\n",
    "        if algorithm_name == 'JAX PSO':\n",
    "            if i == 0:\n",
    "                parallel_jax_pso(objective_fn, bounds, params, num_subswarms)    \n",
    "            start_time = time.perf_counter()\n",
    "            parallel_jax_pso(objective_fn, bounds, params, num_subswarms)\n",
    "        else:\n",
    "            start_time = time.perf_counter()\n",
    "            algorithm_fn(objective_fn, bounds, **params)\n",
    "\n",
    "        elapsed = time.perf_counter() - start_time\n",
    "        execution_times.append(elapsed)\n",
    "\n",
    "        logger.info(\n",
    "            '%s | Run %d/%d | [DONE] in %.6fs',\n",
    "            algorithm_name,\n",
    "            i + 1,\n",
    "            num_runs,\n",
    "            elapsed,\n",
    "        )\n",
    "\n",
    "    return execution_times\n",
    "\n",
    "def compare_pso_implementations(\n",
    "    benchmarks: dict,\n",
    "    algorithms: dict,\n",
    "    dims: list,\n",
    "    num_runs: int,\n",
    "    num_subswarms: int,\n",
    "    params: dict,\n",
    ") -> dict:\n",
    "\n",
    "    logger.info('='*60)\n",
    "    logger.info(\n",
    "        'INITIATING BENCHMARKS | Runs: %d | Subswarms: %d',\n",
    "        num_runs,\n",
    "        num_subswarms,\n",
    "    )\n",
    "    logger.info('='*60)\n",
    "\n",
    "    results = {\n",
    "        dim: {\n",
    "            benchmark_name: {\n",
    "                algorithm_name: [] for algorithm_name in algorithms\n",
    "            } for benchmark_name in benchmarks\n",
    "        } for dim in dims\n",
    "    }\n",
    "\n",
    "    for dim in dims:\n",
    "        params['num_dims'] = dim\n",
    "\n",
    "        for benchmark_name, (objective_fn, bounds) in benchmarks.items():\n",
    "            for name, algorithm in algorithms.items():\n",
    "                logger.info(\n",
    "                    '>>> Dimension: %d | Benchmark: %s | Algorithm: %s',\n",
    "                    dim,\n",
    "                    benchmark_name,\n",
    "                    name,\n",
    "                )\n",
    "\n",
    "                results[dim][benchmark_name][name] = benchmark_algorithm(\n",
    "                    name,\n",
    "                    algorithm,\n",
    "                    objective_fn,\n",
    "                    bounds,\n",
    "                    params,\n",
    "                    num_runs,\n",
    "                    num_subswarms,\n",
    "                )\n",
    "\n",
    "    logger.info('='*60)\n",
    "    logger.info('COMPLETED | All benchmarks finished successfully')\n",
    "    logger.info('='*60)\n",
    "    return results\n",
    "\n",
    "def save_results(results: dict) -> None:\n",
    "    rows = []\n",
    "\n",
    "    for dim, benchmarks_data in results.items():\n",
    "        for benchmark_name, algorithms_data in benchmarks_data.items():\n",
    "            for algorithm_name, execution_times in algorithms_data.items():\n",
    "                mean_time = float(jnp.mean(jnp.array(execution_times)))\n",
    "                std_time = float(jnp.std(jnp.array(execution_times)))\n",
    "\n",
    "                rows.extend(\n",
    "                    [\n",
    "                        {\n",
    "                            'Dimension': dim,\n",
    "                            'Benchmark': benchmark_name,\n",
    "                            'Algorithm': algorithm_name,\n",
    "                            'Execution Times': execution_times,\n",
    "                            'Mean of Execution Times (s)': mean_time,\n",
    "                            'Standard Deviation of Execution Times (s)': std_time,\n",
    "                        },\n",
    "                    ],\n",
    "                )\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv('../data/results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff723e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = compare_pso_implementations(\n",
    "    config['benchmarks'],\n",
    "    config['algorithms'],\n",
    "    config['dims'],\n",
    "    config['num_runs'],\n",
    "    config['num_subswarms'],\n",
    "    config['params'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b5066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gmp-pso-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
